{
  "Version": "6",
  "Year": "2025",
  "Semester": "Fall",
  "project_name": "Reinforcement Learning for Pseudo-Labeling",
  "Objective": "\n            The goal of this project is to develop a reinforcement learning (RL) framework that can automatically \n            assign pseudo-labels to unlabeled data, improving the performance of semi-supervised learning models. \n            The tool will be designed for data scientists and researchers to experiment with different RL strategies \n            to generate high-quality labels while minimizing labeling errors. \n\n            Develop or refine a methodological approach using RL to select unlabeled examples for labeling, balancing \n            exploration (diverse samples) and exploitation (high-confidence samples).   \n\n            Apply the framework to publicly available datasets (e.g., CIFAR-10, MNIST) to \n            evaluate how pseudo-labeling improves model performance under limited labeled data.  \n\n            Integrate the pseudo-labeling methodology into an open-source library to allow other researchers to \n            leverage RL-based semi-supervised labeling for their own datasets.\n            ",
  "Dataset": "\n            Publicly available classification datasets, such as: MNIST or CIFAR-10.\n            ",
  "Rationale": "\n            Semi-supervised learning is widely used when labeled data is limited, but manually labeling large datasets \n            is time-consuming and costly. Pseudo-labeling allows models to leverage unlabeled data by assigning estimated \n            labels, but existing heuristics may propagate errors and reduce model performance. \n\n            By applying reinforcement learning to pseudo-labeling, students can develop an adaptive strategy that selects \n            the most informative unlabeled examples and balances risk and reward in label assignment. This approach \n            reduces labeling errors, improves model accuracy, and provides a scalable solution for leveraging unlabeled \n            datasets in machine learning research.\n            ",
  "Approach": "\n            [Understanding the Reinforcement Learning (RL) Framework] \n            Students will learn how to formulate pseudo-labeling as an RL problem, including:\n\n            - Understanding Markov Decision Process (MDP) assumptions and limitations: markov property, stationarity, limitations.\n            - Understanding sequential decision making: actions affect future states and rewards.\n            - Understanding terminal states and episodic tasks: pseudo-labeling epidode ends when all unlabeled data is processed.\n            - State space design: selecting features of unlabeled examples, model predictions, and uncertainty measures \n            (e.g. [feature1, ..., featuren, softmax_predictions, cross_entropy]).\n            - Action space design: assign a pseudo-label or skip labeling an example (e.g. {0, 1, ..., 9} for MNIST).\n            - Reward design: reward high-confidence correct pseudo-labels and penalize incorrect assignments.\n            - Algorithm selection: evaluate classical RL methods such as Q-learning, and deep RL methods such as Policy Gradient approaches.\n\n            [`model.py` & `train.py`]\n            Students will learn how to adapt existing RL repositories for pseudo-labeling:\n\n            - Utilize existing RL repos (e.g. `twallett` GitHub Repo: `rl-lecture-code`) with existing RL models (e.g. Policy Gradient, PPO Clip).\n            - Train RL agent on existing OpenAI Gymnasium environments (e.g. `CartPole-v1`) just to become familiar with code structure.\n\n            [`utils/env.py` & `test.py`]\n            Students will learn how to build a custom OpenAI Gymnasium environment for pseudo-labeling:\n\n            - PseudoLabelEnv() & __init__() -> class: Initialize Custom OpenAI Gymnasium environment.\n            OpenAI Gymnasium Core Methods:\n            - self.reset() -> method/func: Reset environment to initial state.\n            - self.step(action) -> method/func: Take action (assign pseudo-label or skip) and return next_state, reward, done, info.\n            - (OPTIONAL) self.render() -> method/func: visualization of environment state.\n            - self.close() -> method/func: Clean up resources.\n\n            Additional Custom Pseudo-Labeling Methods:\n                Basic MDP Custom Methods:\n                - self.load_data() -> method/func: Load labeled and unlabeled datasets.\n                - self.split_data() -> method/func: Split data into training and test sets.\n                - self.get_state() -> method/func: Return feature representations for RL state.\n                - self.calculate_reward() -> method/func: Compute reward based on correctness of pseudo-label and downstream model performance.\n\n                Downstream Model Custom Methods:\n                - DownstreamModel() & __init__() -> class: Initialize Downstream DL Model (MLP or CNN).\n                - self.train_downstream_model() -> method/func: Train model on labeled + pseudo-labeled data.\n\n                Evaluation Custom Methods:\n                - evaluate_pseudo_labels() -> method/func: Compare pseudo-labels with ground truth for evaluation.\n                - get_cum_rew() -> method/func: Compute cumulative reward over an episode.\n                - get_classification_metric_gain() -> method/func: Measure improvement in downstream model using pseudo-labels.\n\n            - `test.py` similar to `train.py` but for evaluating trained RL agent on unseen data.\n\n            [`benchmark.py`]\n            Students will learn how to systematically evaluate pseudo-labeling approaches:\n\n            - Run experiments with different RL algorithms and hyperparameters.\n            - Run experiments with different datasets.\n            - Record pseudo-labeling performance and downstream model classification metrics.\n            - Export results for analysis.\n            ",
  "Timeline": "\n            [Understanding RL Framework] 1 week\n            [`model.py` & `train.py`] 2 weeks\n            [`utils/env.py` & `test.py`] 7 weeks\n            [`benchmark.py`] 2 weeks (start writting research paper here)\n            ",
  "Expected Number Students": "\n            Given the scope and complexity of the project, it is recommended to have 2-3 students working collaboratively.\n            ",
  "Research Contributions": "\n            This project will contribute to machine learning research by providing an open-source RL framework for \n            adaptive pseudo-labeling in semi-supervised learning. The methodology will enable researchers to assign \n            high-quality pseudo-labels to unlabeled data, improving model performance and reducing labeling costs. \n            Research findings can be published in academic journals or conferences, and the framework will be made \n            available for future researchers to extend and apply to new datasets.\n            ",
  "Possible Issues": "\n            - Reinforcement Learning concepts can be difficult for students to grasp, especially MDP assumptions.  \n            - Designing state, action, and reward spaces for pseudo-labeling may be non-trivial.  \n            - Training RL agents can be computationally expensive and time-consuming.  \n            - Risk of overfitting to small datasets or poor generalization to new unlabeled data.  \n            - Debugging custom environments and reward functions can be challenging.  \n            - Evaluation of pseudo-label quality requires careful metric design.  \n            ",
  "Proposed by": "Tyler Wallett",
  "Proposed by email": "twallett@gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gwu.edu",
  "collaborator": "None",
  "funding_opportunity": "Open Source Community Project",
  "github_repo": "None"
}